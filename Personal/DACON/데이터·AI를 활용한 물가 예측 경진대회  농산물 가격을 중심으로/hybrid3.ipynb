{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from types import SimpleNamespace\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_size\": 3\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)\n",
    "\n",
    "품목_리스트 = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function for Feature Engineering\n",
    "- 타겟의 필터 조건을 제외한 메타데이터의 필터 조건은 참가자들 각자의 기준에 맞춰 자유롭게 사용가능 \n",
    "- 밑의 필터 조건은 임의로 제공하는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, scaler=None):\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "    # 타겟 및 메타데이터 필터 조건 정의\n",
    "    conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None, \n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # 타겟 데이터 필터링\n",
    "    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "    target_mask = conditions[품목명]['target'](raw_품목)\n",
    "    filtered_data = raw_품목[target_mask]\n",
    "\n",
    "    # 다른 품종에 대한 파생변수 생성\n",
    "    other_data = raw_품목[~target_mask]\n",
    "    unique_combinations = other_data[['품종명', '거래단위', '등급']].drop_duplicates()\n",
    "    for _, row in unique_combinations.iterrows():\n",
    "        품종명, 거래단위, 등급 = row['품종명'], row['거래단위'], row['등급']\n",
    "        mask = (other_data['품종명'] == 품종명) & (other_data['거래단위'] == 거래단위) & (other_data['등급'] == 등급)\n",
    "        temp_df = other_data[mask]\n",
    "        for col in ['평년 평균가격(원)', '평균가격(원)']:\n",
    "            new_col_name = f'{품종명}_{거래단위}_{등급}_{col}'\n",
    "            filtered_data = filtered_data.merge(temp_df[['시점', col]], on='시점', how='left', suffixes=('', f'_{new_col_name}'))\n",
    "            filtered_data.rename(columns={f'{col}_{new_col_name}': new_col_name}, inplace=True)\n",
    "\n",
    "\n",
    "    # 공판장 데이터 처리\n",
    "    if conditions[품목명]['공판장']:\n",
    "        filtered_공판장 = 산지공판장\n",
    "        for key, value in conditions[품목명]['공판장'].items():\n",
    "            filtered_공판장 = filtered_공판장[filtered_공판장[key].isin(value)]\n",
    "        \n",
    "        filtered_공판장 = filtered_공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "\n",
    "    # 도매 데이터 처리\n",
    "    if conditions[품목명]['도매']:\n",
    "        filtered_도매 = 전국도매\n",
    "        for key, value in conditions[품목명]['도매'].items():\n",
    "            filtered_도매 = filtered_도매[filtered_도매[key].isin(value)]\n",
    "        \n",
    "        filtered_도매 = filtered_도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 수치형 컬럼 처리\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data = filtered_data[['시점'] + list(numeric_columns)]\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n",
    "\n",
    "    # 정규화 적용\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgriculturePriceDataset(Dataset):\n",
    "    def __init__(self, dataframe, window_size=9, prediction_length=3, is_test=False):\n",
    "        self.data = dataframe\n",
    "        self.window_size = window_size\n",
    "        self.prediction_length = prediction_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.price_column = [col for col in self.data.columns if '평균가격(원)' in col and len(col.split('_')) == 1][0]\n",
    "        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        self.sequences = []\n",
    "        if not self.is_test:\n",
    "            for i in range(len(self.data) - self.window_size - self.prediction_length + 1):\n",
    "                x = self.data[self.numeric_columns].iloc[i:i+self.window_size].values\n",
    "                y = self.data[self.price_column].iloc[i+self.window_size:i+self.window_size+self.prediction_length].values\n",
    "                self.sequences.append((x, y))\n",
    "        else:\n",
    "            self.sequences = [self.data[self.numeric_columns].values]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_test:\n",
    "            x, y = self.sequences[idx]\n",
    "            return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "        else:\n",
    "            return torch.FloatTensor(self.sequences[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train LSTM model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)  \n",
    "\n",
    "# Function to evaluate LSTM model\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data and return a DataFrame\n",
    "def process_data(raw_file, 품목명, scaler=None):\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "\n",
    "    # Preprocessing here...\n",
    "    numeric_columns = raw_품목.select_dtypes(include=[np.number]).columns\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        raw_품목[numeric_columns] = scaler.fit_transform(raw_품목[numeric_columns])\n",
    "    else:\n",
    "        raw_품목[numeric_columns] = scaler.transform(raw_품목[numeric_columns])\n",
    "\n",
    "    return raw_품목, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 품목명: 감자\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongi\\AppData\\Local\\Temp\\ipykernel_8420\\1998089187.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_품목[numeric_columns] = scaler.fit_transform(raw_품목[numeric_columns])\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0404, Val Loss: 0.0278\n",
      "Epoch 2, Train Loss: 0.0241, Val Loss: 0.0209\n",
      "Epoch 3, Train Loss: 0.0199, Val Loss: 0.0181\n",
      "Epoch 4, Train Loss: 0.0176, Val Loss: 0.0163\n",
      "Epoch 5, Train Loss: 0.0159, Val Loss: 0.0146\n",
      "Epoch 6, Train Loss: 0.0151, Val Loss: 0.0143\n",
      "Epoch 7, Train Loss: 0.0147, Val Loss: 0.0139\n",
      "Epoch 8, Train Loss: 0.0148, Val Loss: 0.0136\n",
      "Epoch 9, Train Loss: 0.0141, Val Loss: 0.0137\n",
      "Epoch 10, Train Loss: 0.0143, Val Loss: 0.0135\n",
      "Processing 품목명: 건고추\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongi\\AppData\\Local\\Temp\\ipykernel_8420\\1998089187.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_품목[numeric_columns] = scaler.fit_transform(raw_품목[numeric_columns])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1626, Val Loss: 0.0712\n",
      "Epoch 2, Train Loss: 0.0684, Val Loss: 0.0669\n",
      "Epoch 3, Train Loss: 0.0640, Val Loss: 0.0628\n",
      "Epoch 4, Train Loss: 0.0584, Val Loss: 0.0526\n",
      "Epoch 5, Train Loss: 0.0563, Val Loss: 0.0538\n",
      "Epoch 6, Train Loss: 0.0544, Val Loss: 0.0517\n",
      "Epoch 7, Train Loss: 0.0528, Val Loss: 0.0511\n",
      "Epoch 8, Train Loss: 0.0509, Val Loss: 0.0539\n",
      "Epoch 9, Train Loss: 0.0522, Val Loss: 0.0565\n",
      "Epoch 10, Train Loss: 0.0522, Val Loss: 0.0550\n",
      "Processing 품목명: 깐마늘(국산)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongi\\AppData\\Local\\Temp\\ipykernel_8420\\1998089187.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_품목[numeric_columns] = scaler.fit_transform(raw_품목[numeric_columns])\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0181, Val Loss: 0.0025\n",
      "Epoch 2, Train Loss: 0.0045, Val Loss: 0.0027\n",
      "Epoch 3, Train Loss: 0.0018, Val Loss: 0.0015\n",
      "Epoch 4, Train Loss: 0.0017, Val Loss: 0.0018\n",
      "Epoch 5, Train Loss: 0.0016, Val Loss: 0.0006\n",
      "Epoch 6, Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch 7, Train Loss: 0.0009, Val Loss: 0.0011\n",
      "Epoch 8, Train Loss: 0.0013, Val Loss: 0.0019\n",
      "Epoch 9, Train Loss: 0.0014, Val Loss: 0.0012\n",
      "Epoch 10, Train Loss: 0.0009, Val Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Training Loop\n",
    "품목별_predictions = {}\n",
    "품목별_scalers = {}\n",
    "\n",
    "for 품목명 in ['감자', '건고추', '깐마늘(국산)']:  # Example 품목명\n",
    "    print(f\"Processing 품목명: {품목명}\")\n",
    "\n",
    "    # Load data\n",
    "    train_data, scaler = process_data(\"./Data/train/train.csv\", 품목명)\n",
    "    품목별_scalers[품목명] = scaler\n",
    "\n",
    "    # Time-Series Forecasting with ARIMA\n",
    "    price_series = train_data['평균가격(원)']\n",
    "    model_arima = ARIMA(price_series, order=(5, 1, 0))  # ARIMA order can be adjusted\n",
    "    arima_fit = model_arima.fit()\n",
    "    arima_predictions = arima_fit.predict(start=len(price_series), end=len(price_series) + len(train_data) - 1)\n",
    "\n",
    "    # Ensure predictions match the length of the test data\n",
    "    arima_predictions = arima_predictions[:len(train_data)].tolist()\n",
    "\n",
    "    # LSTM Training\n",
    "    dataset = AgriculturePriceDataset(train_data)\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "\n",
    "    input_size = len(dataset.numeric_columns)\n",
    "    model = PricePredictionLSTM(input_size, hidden_size=64, num_layers=2, output_size=3)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, 10)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'models/best_model_{품목명}.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    품목별_predictions[품목명] = arima_predictions\n",
    "\n",
    "# Save predictions to submission file\n",
    "sample_submission = pd.read_csv('./sample_submission.csv')\n",
    "\n",
    "for 품목명, predictions in 품목별_predictions.items():\n",
    "    if len(predictions) == len(sample_submission):\n",
    "        sample_submission[품목명] = predictions\n",
    "    else:\n",
    "        sample_submission[품목명] = predictions[:len(sample_submission)]\n",
    "\n",
    "# Save the result\n",
    "sample_submission.to_csv('./baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 품목명: 건고추\n",
      "Epoch 1, Train Loss: 0.7215, Val Loss: 0.7006\n",
      "Epoch 2, Train Loss: 0.7228, Val Loss: 0.6997\n",
      "Epoch 3, Train Loss: 0.7210, Val Loss: 0.6989\n",
      "Epoch 4, Train Loss: 0.7214, Val Loss: 0.6981\n",
      "Epoch 5, Train Loss: 0.7174, Val Loss: 0.6973\n",
      "Epoch 6, Train Loss: 0.7162, Val Loss: 0.6964\n",
      "Epoch 7, Train Loss: 0.7180, Val Loss: 0.6956\n",
      "Epoch 8, Train Loss: 0.7163, Val Loss: 0.6948\n",
      "Epoch 9, Train Loss: 0.7139, Val Loss: 0.6940\n",
      "Epoch 10, Train Loss: 0.7167, Val Loss: 0.6931\n",
      "Epoch 11, Train Loss: 0.7125, Val Loss: 0.6923\n",
      "Epoch 12, Train Loss: 0.7119, Val Loss: 0.6915\n",
      "Epoch 13, Train Loss: 0.7152, Val Loss: 0.6906\n",
      "Epoch 14, Train Loss: 0.7112, Val Loss: 0.6898\n",
      "Epoch 15, Train Loss: 0.7111, Val Loss: 0.6890\n",
      "Epoch 16, Train Loss: 0.7052, Val Loss: 0.6881\n",
      "Epoch 17, Train Loss: 0.7064, Val Loss: 0.6873\n",
      "Epoch 18, Train Loss: 0.7068, Val Loss: 0.6865\n",
      "Epoch 19, Train Loss: 0.7042, Val Loss: 0.6856\n",
      "Epoch 20, Train Loss: 0.7042, Val Loss: 0.6848\n",
      "Epoch 21, Train Loss: 0.7064, Val Loss: 0.6839\n",
      "Epoch 22, Train Loss: 0.7011, Val Loss: 0.6831\n",
      "Epoch 23, Train Loss: 0.6993, Val Loss: 0.6823\n",
      "Epoch 24, Train Loss: 0.6974, Val Loss: 0.6814\n",
      "Epoch 25, Train Loss: 0.7004, Val Loss: 0.6806\n",
      "Epoch 26, Train Loss: 0.7008, Val Loss: 0.6797\n",
      "Epoch 27, Train Loss: 0.6999, Val Loss: 0.6788\n",
      "Epoch 28, Train Loss: 0.6963, Val Loss: 0.6780\n",
      "Epoch 29, Train Loss: 0.6949, Val Loss: 0.6771\n",
      "Epoch 30, Train Loss: 0.6956, Val Loss: 0.6763\n",
      "Processing 품목명: 사과\n",
      "Epoch 1, Train Loss: 0.8430, Val Loss: 0.8513\n",
      "Epoch 2, Train Loss: 0.8405, Val Loss: 0.8503\n",
      "Epoch 3, Train Loss: 0.8463, Val Loss: 0.8493\n",
      "Epoch 4, Train Loss: 0.8405, Val Loss: 0.8484\n",
      "Epoch 5, Train Loss: 0.8428, Val Loss: 0.8474\n",
      "Epoch 6, Train Loss: 0.8387, Val Loss: 0.8464\n",
      "Epoch 7, Train Loss: 0.8344, Val Loss: 0.8455\n",
      "Epoch 8, Train Loss: 0.8385, Val Loss: 0.8445\n",
      "Epoch 9, Train Loss: 0.8385, Val Loss: 0.8436\n",
      "Epoch 10, Train Loss: 0.8360, Val Loss: 0.8426\n",
      "Epoch 11, Train Loss: 0.8314, Val Loss: 0.8417\n",
      "Epoch 12, Train Loss: 0.8375, Val Loss: 0.8408\n",
      "Epoch 13, Train Loss: 0.8365, Val Loss: 0.8398\n",
      "Epoch 14, Train Loss: 0.8313, Val Loss: 0.8389\n",
      "Epoch 15, Train Loss: 0.8334, Val Loss: 0.8379\n",
      "Epoch 16, Train Loss: 0.8329, Val Loss: 0.8370\n",
      "Epoch 17, Train Loss: 0.8281, Val Loss: 0.8361\n",
      "Epoch 18, Train Loss: 0.8302, Val Loss: 0.8351\n",
      "Epoch 19, Train Loss: 0.8287, Val Loss: 0.8342\n",
      "Epoch 20, Train Loss: 0.8294, Val Loss: 0.8332\n",
      "Epoch 21, Train Loss: 0.8275, Val Loss: 0.8323\n",
      "Epoch 22, Train Loss: 0.8236, Val Loss: 0.8313\n",
      "Epoch 23, Train Loss: 0.8245, Val Loss: 0.8304\n",
      "Epoch 24, Train Loss: 0.8250, Val Loss: 0.8294\n",
      "Epoch 25, Train Loss: 0.8226, Val Loss: 0.8285\n",
      "Epoch 26, Train Loss: 0.8219, Val Loss: 0.8275\n",
      "Epoch 27, Train Loss: 0.8219, Val Loss: 0.8265\n",
      "Epoch 28, Train Loss: 0.8199, Val Loss: 0.8256\n",
      "Epoch 29, Train Loss: 0.8173, Val Loss: 0.8246\n",
      "Epoch 30, Train Loss: 0.8182, Val Loss: 0.8236\n",
      "Processing 품목명: 감자\n",
      "Epoch 1, Train Loss: 0.4180, Val Loss: 0.3306\n",
      "Epoch 2, Train Loss: 0.4127, Val Loss: 0.3295\n",
      "Epoch 3, Train Loss: 0.4090, Val Loss: 0.3284\n",
      "Epoch 4, Train Loss: 0.4034, Val Loss: 0.3273\n",
      "Epoch 5, Train Loss: 0.4105, Val Loss: 0.3262\n",
      "Epoch 6, Train Loss: 0.3975, Val Loss: 0.3251\n",
      "Epoch 7, Train Loss: 0.4033, Val Loss: 0.3240\n",
      "Epoch 8, Train Loss: 0.4037, Val Loss: 0.3229\n",
      "Epoch 9, Train Loss: 0.4024, Val Loss: 0.3218\n",
      "Epoch 10, Train Loss: 0.4089, Val Loss: 0.3207\n",
      "Epoch 11, Train Loss: 0.3969, Val Loss: 0.3196\n",
      "Epoch 12, Train Loss: 0.4001, Val Loss: 0.3185\n",
      "Epoch 13, Train Loss: 0.4008, Val Loss: 0.3174\n",
      "Epoch 14, Train Loss: 0.4036, Val Loss: 0.3163\n",
      "Epoch 15, Train Loss: 0.3988, Val Loss: 0.3152\n",
      "Epoch 16, Train Loss: 0.4041, Val Loss: 0.3141\n",
      "Epoch 17, Train Loss: 0.3989, Val Loss: 0.3130\n",
      "Epoch 18, Train Loss: 0.4003, Val Loss: 0.3119\n",
      "Epoch 19, Train Loss: 0.3929, Val Loss: 0.3108\n",
      "Epoch 20, Train Loss: 0.3852, Val Loss: 0.3097\n",
      "Epoch 21, Train Loss: 0.3941, Val Loss: 0.3086\n",
      "Epoch 22, Train Loss: 0.3731, Val Loss: 0.3075\n",
      "Epoch 23, Train Loss: 0.3826, Val Loss: 0.3064\n",
      "Epoch 24, Train Loss: 0.3746, Val Loss: 0.3053\n",
      "Epoch 25, Train Loss: 0.3884, Val Loss: 0.3041\n",
      "Epoch 26, Train Loss: 0.3845, Val Loss: 0.3030\n",
      "Epoch 27, Train Loss: 0.3872, Val Loss: 0.3019\n",
      "Epoch 28, Train Loss: 0.3834, Val Loss: 0.3008\n",
      "Epoch 29, Train Loss: 0.3728, Val Loss: 0.2996\n",
      "Epoch 30, Train Loss: 0.3713, Val Loss: 0.2985\n",
      "Processing 품목명: 배\n",
      "Epoch 1, Train Loss: 0.5283, Val Loss: 0.5575\n",
      "Epoch 2, Train Loss: 0.5212, Val Loss: 0.5567\n",
      "Epoch 3, Train Loss: 0.5117, Val Loss: 0.5558\n",
      "Epoch 4, Train Loss: 0.5177, Val Loss: 0.5549\n",
      "Epoch 5, Train Loss: 0.5148, Val Loss: 0.5540\n",
      "Epoch 6, Train Loss: 0.5183, Val Loss: 0.5532\n",
      "Epoch 7, Train Loss: 0.5204, Val Loss: 0.5523\n",
      "Epoch 8, Train Loss: 0.5224, Val Loss: 0.5514\n",
      "Epoch 9, Train Loss: 0.5137, Val Loss: 0.5505\n",
      "Epoch 10, Train Loss: 0.5210, Val Loss: 0.5497\n",
      "Epoch 11, Train Loss: 0.5076, Val Loss: 0.5488\n",
      "Epoch 12, Train Loss: 0.5192, Val Loss: 0.5479\n",
      "Epoch 13, Train Loss: 0.5086, Val Loss: 0.5471\n",
      "Epoch 14, Train Loss: 0.5146, Val Loss: 0.5462\n",
      "Epoch 15, Train Loss: 0.5042, Val Loss: 0.5453\n",
      "Epoch 16, Train Loss: 0.5081, Val Loss: 0.5445\n",
      "Epoch 17, Train Loss: 0.4981, Val Loss: 0.5436\n",
      "Epoch 18, Train Loss: 0.5068, Val Loss: 0.5427\n",
      "Epoch 19, Train Loss: 0.5076, Val Loss: 0.5419\n",
      "Epoch 20, Train Loss: 0.5152, Val Loss: 0.5410\n",
      "Epoch 21, Train Loss: 0.4989, Val Loss: 0.5401\n",
      "Epoch 22, Train Loss: 0.5047, Val Loss: 0.5393\n",
      "Epoch 23, Train Loss: 0.5044, Val Loss: 0.5384\n",
      "Epoch 24, Train Loss: 0.4962, Val Loss: 0.5375\n",
      "Epoch 25, Train Loss: 0.4942, Val Loss: 0.5367\n",
      "Epoch 26, Train Loss: 0.5007, Val Loss: 0.5358\n",
      "Epoch 27, Train Loss: 0.4987, Val Loss: 0.5349\n",
      "Epoch 28, Train Loss: 0.4966, Val Loss: 0.5341\n",
      "Epoch 29, Train Loss: 0.4931, Val Loss: 0.5332\n",
      "Epoch 30, Train Loss: 0.5006, Val Loss: 0.5323\n",
      "Processing 품목명: 깐마늘(국산)\n",
      "Epoch 1, Train Loss: 0.0780, Val Loss: 0.0772\n",
      "Epoch 2, Train Loss: 0.0772, Val Loss: 0.0763\n",
      "Epoch 3, Train Loss: 0.0763, Val Loss: 0.0755\n",
      "Epoch 4, Train Loss: 0.0755, Val Loss: 0.0747\n",
      "Epoch 5, Train Loss: 0.0747, Val Loss: 0.0738\n",
      "Epoch 6, Train Loss: 0.0738, Val Loss: 0.0730\n",
      "Epoch 7, Train Loss: 0.0729, Val Loss: 0.0722\n",
      "Epoch 8, Train Loss: 0.0721, Val Loss: 0.0713\n",
      "Epoch 9, Train Loss: 0.0713, Val Loss: 0.0705\n",
      "Epoch 10, Train Loss: 0.0704, Val Loss: 0.0697\n",
      "Epoch 11, Train Loss: 0.0696, Val Loss: 0.0689\n",
      "Epoch 12, Train Loss: 0.0688, Val Loss: 0.0681\n",
      "Epoch 13, Train Loss: 0.0680, Val Loss: 0.0673\n",
      "Epoch 14, Train Loss: 0.0672, Val Loss: 0.0666\n",
      "Epoch 15, Train Loss: 0.0664, Val Loss: 0.0659\n",
      "Epoch 16, Train Loss: 0.0657, Val Loss: 0.0652\n",
      "Epoch 17, Train Loss: 0.0649, Val Loss: 0.0645\n",
      "Epoch 18, Train Loss: 0.0642, Val Loss: 0.0638\n",
      "Epoch 19, Train Loss: 0.0634, Val Loss: 0.0632\n",
      "Epoch 20, Train Loss: 0.0628, Val Loss: 0.0625\n",
      "Epoch 21, Train Loss: 0.0620, Val Loss: 0.0619\n",
      "Epoch 22, Train Loss: 0.0613, Val Loss: 0.0612\n",
      "Epoch 23, Train Loss: 0.0607, Val Loss: 0.0605\n",
      "Epoch 24, Train Loss: 0.0600, Val Loss: 0.0599\n",
      "Epoch 25, Train Loss: 0.0592, Val Loss: 0.0592\n",
      "Epoch 26, Train Loss: 0.0585, Val Loss: 0.0585\n",
      "Epoch 27, Train Loss: 0.0579, Val Loss: 0.0578\n",
      "Epoch 28, Train Loss: 0.0572, Val Loss: 0.0571\n",
      "Epoch 29, Train Loss: 0.0565, Val Loss: 0.0565\n",
      "Epoch 30, Train Loss: 0.0557, Val Loss: 0.0558\n",
      "Processing 품목명: 무\n",
      "Epoch 1, Train Loss: 0.0776, Val Loss: 0.0758\n",
      "Epoch 2, Train Loss: 0.0766, Val Loss: 0.0748\n",
      "Epoch 3, Train Loss: 0.0757, Val Loss: 0.0738\n",
      "Epoch 4, Train Loss: 0.0747, Val Loss: 0.0728\n",
      "Epoch 5, Train Loss: 0.0736, Val Loss: 0.0719\n",
      "Epoch 6, Train Loss: 0.0728, Val Loss: 0.0709\n",
      "Epoch 7, Train Loss: 0.0716, Val Loss: 0.0700\n",
      "Epoch 8, Train Loss: 0.0707, Val Loss: 0.0691\n",
      "Epoch 9, Train Loss: 0.0700, Val Loss: 0.0681\n",
      "Epoch 10, Train Loss: 0.0689, Val Loss: 0.0672\n",
      "Epoch 11, Train Loss: 0.0681, Val Loss: 0.0663\n",
      "Epoch 12, Train Loss: 0.0671, Val Loss: 0.0654\n",
      "Epoch 13, Train Loss: 0.0664, Val Loss: 0.0645\n",
      "Epoch 14, Train Loss: 0.0653, Val Loss: 0.0636\n",
      "Epoch 15, Train Loss: 0.0645, Val Loss: 0.0628\n",
      "Epoch 16, Train Loss: 0.0638, Val Loss: 0.0619\n",
      "Epoch 17, Train Loss: 0.0628, Val Loss: 0.0612\n",
      "Epoch 18, Train Loss: 0.0619, Val Loss: 0.0604\n",
      "Epoch 19, Train Loss: 0.0613, Val Loss: 0.0597\n",
      "Epoch 20, Train Loss: 0.0605, Val Loss: 0.0589\n",
      "Epoch 21, Train Loss: 0.0597, Val Loss: 0.0581\n",
      "Epoch 22, Train Loss: 0.0588, Val Loss: 0.0574\n",
      "Epoch 23, Train Loss: 0.0582, Val Loss: 0.0566\n",
      "Epoch 24, Train Loss: 0.0573, Val Loss: 0.0558\n",
      "Epoch 25, Train Loss: 0.0566, Val Loss: 0.0550\n",
      "Epoch 26, Train Loss: 0.0557, Val Loss: 0.0543\n",
      "Epoch 27, Train Loss: 0.0549, Val Loss: 0.0535\n",
      "Epoch 28, Train Loss: 0.0542, Val Loss: 0.0527\n",
      "Epoch 29, Train Loss: 0.0533, Val Loss: 0.0519\n",
      "Epoch 30, Train Loss: 0.0526, Val Loss: 0.0511\n",
      "Processing 품목명: 상추\n",
      "Epoch 1, Train Loss: 0.3164, Val Loss: 0.3396\n",
      "Epoch 2, Train Loss: 0.3196, Val Loss: 0.3390\n",
      "Epoch 3, Train Loss: 0.3132, Val Loss: 0.3384\n",
      "Epoch 4, Train Loss: 0.3145, Val Loss: 0.3377\n",
      "Epoch 5, Train Loss: 0.3129, Val Loss: 0.3371\n",
      "Epoch 6, Train Loss: 0.3062, Val Loss: 0.3364\n",
      "Epoch 7, Train Loss: 0.3064, Val Loss: 0.3358\n",
      "Epoch 8, Train Loss: 0.3083, Val Loss: 0.3351\n",
      "Epoch 9, Train Loss: 0.3065, Val Loss: 0.3345\n",
      "Epoch 10, Train Loss: 0.2963, Val Loss: 0.3339\n",
      "Epoch 11, Train Loss: 0.3071, Val Loss: 0.3332\n",
      "Epoch 12, Train Loss: 0.3085, Val Loss: 0.3326\n",
      "Epoch 13, Train Loss: 0.3081, Val Loss: 0.3319\n",
      "Epoch 14, Train Loss: 0.2986, Val Loss: 0.3313\n",
      "Epoch 15, Train Loss: 0.3012, Val Loss: 0.3306\n",
      "Epoch 16, Train Loss: 0.3108, Val Loss: 0.3300\n",
      "Epoch 17, Train Loss: 0.3030, Val Loss: 0.3294\n",
      "Epoch 18, Train Loss: 0.2977, Val Loss: 0.3287\n",
      "Epoch 19, Train Loss: 0.3052, Val Loss: 0.3281\n",
      "Epoch 20, Train Loss: 0.3064, Val Loss: 0.3274\n",
      "Epoch 21, Train Loss: 0.2984, Val Loss: 0.3268\n",
      "Epoch 22, Train Loss: 0.3020, Val Loss: 0.3261\n",
      "Epoch 23, Train Loss: 0.2852, Val Loss: 0.3255\n",
      "Epoch 24, Train Loss: 0.2998, Val Loss: 0.3249\n",
      "Epoch 25, Train Loss: 0.2928, Val Loss: 0.3242\n",
      "Epoch 26, Train Loss: 0.2861, Val Loss: 0.3236\n",
      "Epoch 27, Train Loss: 0.3024, Val Loss: 0.3229\n",
      "Epoch 28, Train Loss: 0.2879, Val Loss: 0.3223\n",
      "Epoch 29, Train Loss: 0.3157, Val Loss: 0.3216\n",
      "Epoch 30, Train Loss: 0.2874, Val Loss: 0.3210\n",
      "Processing 품목명: 배추\n",
      "Epoch 1, Train Loss: 0.3479, Val Loss: 0.3159\n",
      "Epoch 2, Train Loss: 0.3409, Val Loss: 0.3147\n",
      "Epoch 3, Train Loss: 0.3297, Val Loss: 0.3135\n",
      "Epoch 4, Train Loss: 0.3273, Val Loss: 0.3123\n",
      "Epoch 5, Train Loss: 0.3358, Val Loss: 0.3112\n",
      "Epoch 6, Train Loss: 0.3325, Val Loss: 0.3100\n",
      "Epoch 7, Train Loss: 0.3343, Val Loss: 0.3088\n",
      "Epoch 8, Train Loss: 0.3239, Val Loss: 0.3076\n",
      "Epoch 9, Train Loss: 0.3316, Val Loss: 0.3064\n",
      "Epoch 10, Train Loss: 0.3339, Val Loss: 0.3052\n",
      "Epoch 11, Train Loss: 0.3292, Val Loss: 0.3040\n",
      "Epoch 12, Train Loss: 0.3248, Val Loss: 0.3028\n",
      "Epoch 13, Train Loss: 0.3180, Val Loss: 0.3017\n",
      "Epoch 14, Train Loss: 0.3137, Val Loss: 0.3005\n",
      "Epoch 15, Train Loss: 0.3201, Val Loss: 0.2993\n",
      "Epoch 16, Train Loss: 0.3176, Val Loss: 0.2981\n",
      "Epoch 17, Train Loss: 0.3184, Val Loss: 0.2969\n",
      "Epoch 18, Train Loss: 0.3187, Val Loss: 0.2958\n",
      "Epoch 19, Train Loss: 0.3093, Val Loss: 0.2946\n",
      "Epoch 20, Train Loss: 0.3081, Val Loss: 0.2934\n",
      "Epoch 21, Train Loss: 0.3162, Val Loss: 0.2922\n",
      "Epoch 22, Train Loss: 0.3123, Val Loss: 0.2910\n",
      "Epoch 23, Train Loss: 0.3064, Val Loss: 0.2898\n",
      "Epoch 24, Train Loss: 0.3068, Val Loss: 0.2886\n",
      "Epoch 25, Train Loss: 0.3107, Val Loss: 0.2875\n",
      "Epoch 26, Train Loss: 0.3109, Val Loss: 0.2863\n",
      "Epoch 27, Train Loss: 0.3037, Val Loss: 0.2851\n",
      "Epoch 28, Train Loss: 0.3002, Val Loss: 0.2838\n",
      "Epoch 29, Train Loss: 0.3001, Val Loss: 0.2826\n",
      "Epoch 30, Train Loss: 0.3021, Val Loss: 0.2814\n",
      "Processing 품목명: 양파\n",
      "Epoch 1, Train Loss: 0.4304, Val Loss: 0.4329\n",
      "Epoch 2, Train Loss: 0.4266, Val Loss: 0.4319\n",
      "Epoch 3, Train Loss: 0.4255, Val Loss: 0.4310\n",
      "Epoch 4, Train Loss: 0.4306, Val Loss: 0.4300\n",
      "Epoch 5, Train Loss: 0.4310, Val Loss: 0.4291\n",
      "Epoch 6, Train Loss: 0.4284, Val Loss: 0.4282\n",
      "Epoch 7, Train Loss: 0.4229, Val Loss: 0.4272\n",
      "Epoch 8, Train Loss: 0.4215, Val Loss: 0.4263\n",
      "Epoch 9, Train Loss: 0.4226, Val Loss: 0.4253\n",
      "Epoch 10, Train Loss: 0.4242, Val Loss: 0.4244\n",
      "Epoch 11, Train Loss: 0.4247, Val Loss: 0.4235\n",
      "Epoch 12, Train Loss: 0.4227, Val Loss: 0.4225\n",
      "Epoch 13, Train Loss: 0.4230, Val Loss: 0.4216\n",
      "Epoch 14, Train Loss: 0.4158, Val Loss: 0.4207\n",
      "Epoch 15, Train Loss: 0.4174, Val Loss: 0.4197\n",
      "Epoch 16, Train Loss: 0.4178, Val Loss: 0.4188\n",
      "Epoch 17, Train Loss: 0.4178, Val Loss: 0.4179\n",
      "Epoch 18, Train Loss: 0.4155, Val Loss: 0.4169\n",
      "Epoch 19, Train Loss: 0.4134, Val Loss: 0.4160\n",
      "Epoch 20, Train Loss: 0.4130, Val Loss: 0.4151\n",
      "Epoch 21, Train Loss: 0.4094, Val Loss: 0.4141\n",
      "Epoch 22, Train Loss: 0.4075, Val Loss: 0.4132\n",
      "Epoch 23, Train Loss: 0.4108, Val Loss: 0.4122\n",
      "Epoch 24, Train Loss: 0.4157, Val Loss: 0.4113\n",
      "Epoch 25, Train Loss: 0.4123, Val Loss: 0.4103\n",
      "Epoch 26, Train Loss: 0.4086, Val Loss: 0.4094\n",
      "Epoch 27, Train Loss: 0.4093, Val Loss: 0.4084\n",
      "Epoch 28, Train Loss: 0.4097, Val Loss: 0.4075\n",
      "Epoch 29, Train Loss: 0.4034, Val Loss: 0.4065\n",
      "Epoch 30, Train Loss: 0.4012, Val Loss: 0.4056\n",
      "Processing 품목명: 대파\n",
      "Epoch 1, Train Loss: 0.4789, Val Loss: 0.4306\n",
      "Epoch 2, Train Loss: 0.4819, Val Loss: 0.4296\n",
      "Epoch 3, Train Loss: 0.4733, Val Loss: 0.4286\n",
      "Epoch 4, Train Loss: 0.4748, Val Loss: 0.4276\n",
      "Epoch 5, Train Loss: 0.4783, Val Loss: 0.4266\n",
      "Epoch 6, Train Loss: 0.4706, Val Loss: 0.4255\n",
      "Epoch 7, Train Loss: 0.4686, Val Loss: 0.4245\n",
      "Epoch 8, Train Loss: 0.4760, Val Loss: 0.4235\n",
      "Epoch 9, Train Loss: 0.4650, Val Loss: 0.4225\n",
      "Epoch 10, Train Loss: 0.4639, Val Loss: 0.4215\n",
      "Epoch 11, Train Loss: 0.4691, Val Loss: 0.4204\n",
      "Epoch 12, Train Loss: 0.4689, Val Loss: 0.4194\n",
      "Epoch 13, Train Loss: 0.4649, Val Loss: 0.4184\n",
      "Epoch 14, Train Loss: 0.4571, Val Loss: 0.4174\n",
      "Epoch 15, Train Loss: 0.4633, Val Loss: 0.4163\n",
      "Epoch 16, Train Loss: 0.4613, Val Loss: 0.4153\n",
      "Epoch 17, Train Loss: 0.4618, Val Loss: 0.4143\n",
      "Epoch 18, Train Loss: 0.4593, Val Loss: 0.4133\n",
      "Epoch 19, Train Loss: 0.4610, Val Loss: 0.4122\n",
      "Epoch 20, Train Loss: 0.4564, Val Loss: 0.4112\n",
      "Epoch 21, Train Loss: 0.4560, Val Loss: 0.4101\n",
      "Epoch 22, Train Loss: 0.4594, Val Loss: 0.4091\n",
      "Epoch 23, Train Loss: 0.4612, Val Loss: 0.4081\n",
      "Epoch 24, Train Loss: 0.4564, Val Loss: 0.4070\n",
      "Epoch 25, Train Loss: 0.4562, Val Loss: 0.4060\n",
      "Epoch 26, Train Loss: 0.4533, Val Loss: 0.4050\n",
      "Epoch 27, Train Loss: 0.4526, Val Loss: 0.4039\n",
      "Epoch 28, Train Loss: 0.4402, Val Loss: 0.4028\n",
      "Epoch 29, Train Loss: 0.4457, Val Loss: 0.4018\n",
      "Epoch 30, Train Loss: 0.4507, Val Loss: 0.4007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# ARIMA or any Time Series Forecasting\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_size\": 3\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)\n",
    "품목_리스트 = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']\n",
    "\n",
    "# LSTM Model Class\n",
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class AgriculturePriceDataset(Dataset):\n",
    "    def __init__(self, dataframe, window_size=9, prediction_length=3, is_test=False):\n",
    "        self.data = dataframe\n",
    "        self.window_size = window_size\n",
    "        self.prediction_length = prediction_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.price_column = [col for col in self.data.columns if '평균가격(원)' in col and len(col.split('_')) == 1][0]\n",
    "        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        self.sequences = []\n",
    "        if not self.is_test:\n",
    "            for i in range(len(self.data) - self.window_size - self.prediction_length + 1):\n",
    "                x = self.data[self.numeric_columns].iloc[i:i+self.window_size].values\n",
    "                y = self.data[self.price_column].iloc[i+self.window_size:i+self.window_size+self.prediction_length].values\n",
    "                self.sequences.append((x, y))\n",
    "        else:\n",
    "            self.sequences = [self.data[self.numeric_columns].values]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_test:\n",
    "            x, y = self.sequences[idx]\n",
    "            return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "        else:\n",
    "            return torch.FloatTensor(self.sequences[idx])\n",
    "\n",
    "# Function to train LSTM model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)  \n",
    "\n",
    "# Function to evaluate LSTM model\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "# Function to preprocess data and return a DataFrame\n",
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, scaler=None):\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "    # 타겟 및 메타데이터 필터 조건 정의\n",
    "    conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None,\n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # 타겟 데이터 필터링\n",
    "    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "    target_mask = conditions[품목명]['target'](raw_품목)\n",
    "    filtered_data = raw_품목[target_mask]\n",
    "\n",
    "    # 다른 품종에 대한 파생변수 생성\n",
    "    other_data = raw_품목[~target_mask]\n",
    "    unique_combinations = other_data[['품종명', '거래단위', '등급']].drop_duplicates()\n",
    "    for _, row in unique_combinations.iterrows():\n",
    "        품종명, 거래단위, 등급 = row['품종명'], row['거래단위'], row['등급']\n",
    "        mask = (other_data['품종명'] == 품종명) & (other_data['거래단위'] == 거래단위) & (other_data['등급'] == 등급)\n",
    "        temp_df = other_data[mask]\n",
    "        for col in ['평년 평균가격(원)', '평균가격(원)']:\n",
    "            new_col_name = f'{품종명}_{거래단위}_{등급}_{col}'\n",
    "            filtered_data = filtered_data.merge(temp_df[['시점', col]], on='시점', how='left', suffixes=('', f'_{new_col_name}'))\n",
    "            filtered_data.rename(columns={f'{col}_{new_col_name}': new_col_name}, inplace=True)\n",
    "\n",
    "    # 공판장 데이터 처리\n",
    "    if conditions[품목명]['공판장']:\n",
    "        filtered_공판장 = 산지공판장\n",
    "        for key, value in conditions[품목명]['공판장'].items():\n",
    "            filtered_공판장 = filtered_공판장[filtered_공판장[key].isin(value)]\n",
    "        \n",
    "        filtered_공판장 = filtered_공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "\n",
    "    # 도매 데이터 처리\n",
    "    if conditions[품목명]['도매']:\n",
    "        filtered_도매 = 전국도매\n",
    "        for key, value in conditions[품목명]['도매'].items():\n",
    "            filtered_도매 = filtered_도매[filtered_도매[key].isin(value)]\n",
    "        \n",
    "        filtered_도매 = filtered_도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 수치형 컬럼 처리\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data = filtered_data[['시점'] + list(numeric_columns)]\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n",
    "\n",
    "    # 정규화 적용\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler\n",
    "\n",
    "# Hybrid Training Loop\n",
    "품목별_predictions = {}\n",
    "품목별_scalers = {}\n",
    "\n",
    "for 품목명 in 품목_리스트:\n",
    "    print(f\"Processing 품목명: {품목명}\")\n",
    "\n",
    "    # Load data\n",
    "    train_data, scaler = process_data(\"./Data/train/train.csv\", \"./Data/train/meta/TRAIN_산지공판장_2018-2021.csv\", \"./Data/train/meta/TRAIN_전국도매_2018-2021.csv\", 품목명)\n",
    "    품목별_scalers[품목명] = scaler\n",
    "\n",
    "    # Time-Series Forecasting with ARIMA\n",
    "    price_series = train_data['평균가격(원)']\n",
    "    model_arima = ARIMA(price_series, order=(5, 1, 0))  # ARIMA order can be adjusted\n",
    "    arima_fit = model_arima.fit()\n",
    "    arima_predictions = arima_fit.predict(start=len(price_series), end=len(price_series) + len(train_data) - 1)\n",
    "\n",
    "    # Ensure predictions match the length of the test data\n",
    "    arima_predictions = arima_predictions[:len(train_data)].tolist()\n",
    "\n",
    "    # LSTM Training\n",
    "    dataset = AgriculturePriceDataset(train_data)\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=CFG.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "    input_size = len(dataset.numeric_columns)\n",
    "    model = PricePredictionLSTM(input_size, hidden_size=CFG.hidden_size, num_layers=CFG.num_layers, output_size=CFG.output_size)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    for epoch in range(CFG.epoch):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, CFG.epoch)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'models/best_model_{품목명}.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    품목별_predictions[품목명] = arima_predictions\n",
    "\n",
    "# Save predictions to submission file\n",
    "sample_submission = pd.read_csv('./sample_submission.csv')\n",
    "\n",
    "for 품목명, predictions in 품목별_predictions.items():\n",
    "    if len(predictions) == len(sample_submission):\n",
    "        sample_submission[품목명] = predictions\n",
    "    else:\n",
    "        sample_submission[품목명] = predictions[:len(sample_submission)]\n",
    "\n",
    "# Save the result\n",
    "sample_submission.to_csv('./baseline_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
