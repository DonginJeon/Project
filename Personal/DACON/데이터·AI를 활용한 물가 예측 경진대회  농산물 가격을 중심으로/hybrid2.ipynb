{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from types import SimpleNamespace\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_size\": 3\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)\n",
    "\n",
    "품목_리스트 = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function for Feature Engineering\n",
    "- 타겟의 필터 조건을 제외한 메타데이터의 필터 조건은 참가자들 각자의 기준에 맞춰 자유롭게 사용가능 \n",
    "- 밑의 필터 조건은 임의로 제공하는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, scaler=None):\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "    # 타겟 및 메타데이터 필터 조건 정의\n",
    "    conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None, \n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # 타겟 데이터 필터링\n",
    "    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "    target_mask = conditions[품목명]['target'](raw_품목)\n",
    "    filtered_data = raw_품목[target_mask]\n",
    "\n",
    "    # 다른 품종에 대한 파생변수 생성 및 병합\n",
    "    other_data = raw_품목[~target_mask]\n",
    "    unique_combinations = other_data[['품종명', '거래단위', '등급']].drop_duplicates()\n",
    "    for _, row in unique_combinations.iterrows():\n",
    "        품종명, 거래단위, 등급 = row['품종명'], row['거래단위'], row['등급']\n",
    "        mask = (other_data['품종명'] == 품종명) & (other_data['거래단위'] == 거래단위) & (other_data['등급'] == 등급)\n",
    "        temp_df = other_data[mask]\n",
    "        for col in ['평년 평균가격(원)', '평균가격(원)']:\n",
    "            new_col_name = f'{품종명}_{거래단위}_{등급}_{col}'\n",
    "            filtered_data = filtered_data.merge(temp_df[['시점', col]], on='시점', how='left', suffixes=('', f'_{new_col_name}'))\n",
    "            filtered_data.rename(columns={f'{col}_{new_col_name}': new_col_name}, inplace=True)\n",
    "\n",
    "    # 공판장 데이터 병합\n",
    "    if conditions[품목명]['공판장']:\n",
    "        filtered_공판장 = 산지공판장\n",
    "        for key, value in conditions[품목명]['공판장'].items():\n",
    "            filtered_공판장 = filtered_공판장[filtered_공판장[key].isin(value)]\n",
    "        filtered_공판장 = filtered_공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "\n",
    "    # 도매 데이터 병합\n",
    "    if conditions[품목명]['도매']:\n",
    "        filtered_도매 = 전국도매\n",
    "        for key, value in conditions[품목명]['도매'].items():\n",
    "            filtered_도매 = filtered_도매[filtered_도매[key].isin(value)]\n",
    "        filtered_도매 = filtered_도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 결측값 처리 및 수치형 데이터 정규화\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgriculturePriceDataset(Dataset):\n",
    "    def __init__(self, dataframe, window_size=9, prediction_length=3, is_test=False):\n",
    "        self.data = dataframe\n",
    "        self.window_size = window_size\n",
    "        self.prediction_length = prediction_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # '평균가격(원)' 관련 열을 찾는 로직 개선\n",
    "        price_columns = [col for col in self.data.columns if '평균가격(원)' in col]\n",
    "        \n",
    "        if len(price_columns) == 1:\n",
    "            # '평균가격(원)' 열이 1개일 때 사용\n",
    "            self.price_column = price_columns[0]\n",
    "        elif len(price_columns) > 1:\n",
    "            # '평균가격(원)' 열이 여러 개일 때 우선적으로 사용할 열을 명시적으로 지정\n",
    "            print(f\"여러 개의 '평균가격(원)' 열이 발견되었습니다: {price_columns}\")\n",
    "            # 예를 들어, 첫 번째 열을 사용한다고 가정\n",
    "            self.price_column = price_columns[0]\n",
    "            print(f\"첫 번째 열을 선택합니다: {self.price_column}\")\n",
    "        else:\n",
    "            raise ValueError(\"데이터프레임에 '평균가격(원)'과 관련된 열이 존재하지 않습니다.\")\n",
    "        \n",
    "        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        self.sequences = []\n",
    "        if not self.is_test:\n",
    "            # 가능한 시퀀스를 추출\n",
    "            for i in range(len(self.data) - self.window_size - self.prediction_length + 1):\n",
    "                x = self.data[self.numeric_columns].iloc[i:i+self.window_size].values\n",
    "                y = self.data[self.price_column].iloc[i+self.window_size:i+self.window_size+self.prediction_length].values\n",
    "                self.sequences.append((x, y))\n",
    "        else:\n",
    "            # 테스트 데이터일 때는 시퀀스만 생성 (예측할 타겟 데이터는 없음)\n",
    "            if len(self.data) >= self.window_size:\n",
    "                self.sequences = [self.data[self.numeric_columns].iloc[-self.window_size:].values]\n",
    "            else:\n",
    "                raise ValueError(\"테스트 데이터의 길이가 window_size보다 작습니다.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_test:\n",
    "            x, y = self.sequences[idx]\n",
    "            return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "        else:\n",
    "            # 테스트 데이터에서는 타겟 없이 입력 시퀀스만 반환\n",
    "            x = self.sequences[idx]\n",
    "            return torch.FloatTensor(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# LSTM 모델 정의\n",
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, lstm_output_size):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_output_size = lstm_output_size\n",
    "        \n",
    "        # LSTM 레이어\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # LSTM의 마지막 출력을 위한 선형 레이어\n",
    "        self.fc = nn.Linear(hidden_size, lstm_output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 타임스텝의 출력만 사용\n",
    "        return out\n",
    "\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(next(model.parameters()).device)\n",
    "            batch_y = batch_y.to(next(model.parameters()).device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# LGBM과 LSTM의 Hybrid 모델을 위한 데이터 준비\n",
    "def prepare_hybrid_data(lstm_model, dataloader):\n",
    "    lstm_model.eval()\n",
    "    lstm_features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(next(lstm_model.parameters()).device)\n",
    "            lstm_output = lstm_model(batch_x)\n",
    "            lstm_features.append(lstm_output.cpu().numpy())  # LSTM의 출력을 저장\n",
    "            labels.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    lstm_features = np.concatenate(lstm_features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0).flatten()  # Ensure labels are 1-D\n",
    "    \n",
    "    return lstm_features, labels\n",
    "\n",
    "# LGBM 학습 함수\n",
    "def train_lgbm(lstm_model, train_loader, test_loader, num_boost_round=100, early_stopping_rounds=10):\n",
    "    # LSTM을 사용해 시계열 데이터를 처리하고 특징을 추출\n",
    "    X_train, y_train = prepare_hybrid_data(lstm_model, train_loader)\n",
    "    X_test, y_test = prepare_hybrid_data(lstm_model, test_loader)\n",
    "    \n",
    "    # Ensure y_train and y_test are 1-D arrays\n",
    "    y_train = y_train.flatten()  # Ensure it's a 1-D array\n",
    "    y_test = y_test.flatten()  # Ensure it's a 1-D array\n",
    "    \n",
    "    # LGBM 데이터 세트 생성\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test)\n",
    "    \n",
    "    # LGBM 하이퍼파라미터 설정\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "    # Manually implementing early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_iteration = 0\n",
    "    lgbm_model = None  # Initialize model to None\n",
    "\n",
    "    for iteration in range(num_boost_round):\n",
    "        lgbm_model = lgb.train(params, train_data, num_boost_round=1, init_model=lgbm_model)\n",
    "        val_loss = lgbm_model.best_score['valid']['rmse']  # Access the validation score\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_iteration = iteration + 1  # +1 because iteration is zero-based\n",
    "\n",
    "        \n",
    "        if iteration + 1 - best_iteration >= early_stopping_rounds:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        \n",
    "    return lgbm_model\n",
    "\n",
    "\n",
    "# 최종 예측\n",
    "def predict_hybrid(lstm_model, lgbm_model, dataloader):\n",
    "    lstm_model.eval()\n",
    "    lstm_features, _ = prepare_hybrid_data(lstm_model, dataloader)\n",
    "    \n",
    "    # LGBM을 통해 최종 예측 수행\n",
    "    predictions = lgbm_model.predict(lstm_features)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def train_hybrid_model(lstm_model, train_loader, num_epochs, criterion, optimizer):\n",
    "    lstm_features, lstm_labels = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(next(lstm_model.parameters()).device), batch_y.to(next(lstm_model.parameters()).device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = lstm_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            lstm_features.append(outputs.cpu().detach().numpy())\n",
    "            lstm_labels.append(batch_y.cpu().detach().numpy())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "    \n",
    "    return lstm_features, lstm_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d344f8d9164438b8387ee405408fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "품목 처리 중:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여러 개의 '평균가격(원)' 열이 발견되었습니다: ['평년 평균가격(원)', '평균가격(원)', '햇산양건_30 kg_상품_평년 평균가격(원)', '햇산양건_30 kg_상품_평균가격(원)', '햇산화건_30 kg_중품_평년 평균가격(원)', '햇산화건_30 kg_중품_평균가격(원)', '햇산화건_30 kg_상품_평년 평균가격(원)', '햇산화건_30 kg_상품_평균가격(원)', '양건_30 kg_중품_평년 평균가격(원)', '양건_30 kg_중품_평균가격(원)', '양건_30 kg_상품_평년 평균가격(원)', '양건_30 kg_상품_평균가격(원)', '화건_30 kg_중품_평년 평균가격(원)', '화건_30 kg_중품_평균가격(원)', '햇산양건_30 kg_중품_평년 평균가격(원)', '햇산양건_30 kg_중품_평균가격(원)']\n",
      "첫 번째 열을 선택합니다: 평년 평균가격(원)\n",
      "Epoch 1/30, Val Loss: 0.8227\n",
      "Epoch [1/1], Loss: 0.8418\n",
      "Epoch 2/30, Val Loss: 0.8214\n",
      "Epoch [1/2], Loss: 0.8397\n",
      "Epoch [2/2], Loss: 0.8346\n",
      "Epoch 3/30, Val Loss: 0.8189\n",
      "Epoch [1/3], Loss: 0.8359\n",
      "Epoch [2/3], Loss: 0.8387\n",
      "Epoch [3/3], Loss: 0.8331\n",
      "Epoch 4/30, Val Loss: 0.8152\n",
      "Epoch [1/4], Loss: 0.8303\n",
      "Epoch [2/4], Loss: 0.8301\n",
      "Epoch [3/4], Loss: 0.8322\n",
      "Epoch [4/4], Loss: 0.8316\n",
      "Epoch 5/30, Val Loss: 0.8102\n",
      "Epoch [1/5], Loss: 0.8276\n",
      "Epoch [2/5], Loss: 0.8283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([27, 3])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([64, 3])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\dongi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([42, 3])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.8232\n",
      "Epoch [4/5], Loss: 0.8293\n",
      "Epoch [5/5], Loss: 0.8255\n",
      "Epoch 6/30, Val Loss: 0.8039\n",
      "Epoch [1/6], Loss: 0.8200\n",
      "Epoch [2/6], Loss: 0.8171\n",
      "Epoch [3/6], Loss: 0.8199\n",
      "Epoch [4/6], Loss: 0.8193\n",
      "Epoch [5/6], Loss: 0.8196\n",
      "Epoch [6/6], Loss: 0.8128\n",
      "Epoch 7/30, Val Loss: 0.7963\n",
      "Epoch [1/7], Loss: 0.8125\n",
      "Epoch [2/7], Loss: 0.8130\n",
      "Epoch [3/7], Loss: 0.8103\n",
      "Epoch [4/7], Loss: 0.8125\n",
      "Epoch [5/7], Loss: 0.8102\n",
      "Epoch [6/7], Loss: 0.8094\n",
      "Epoch [7/7], Loss: 0.8057\n",
      "Epoch 8/30, Val Loss: 0.7873\n",
      "Epoch [1/8], Loss: 0.8075\n",
      "Epoch [2/8], Loss: 0.8017\n",
      "Epoch [3/8], Loss: 0.8044\n",
      "Epoch [4/8], Loss: 0.8006\n",
      "Epoch [5/8], Loss: 0.7996\n",
      "Epoch [6/8], Loss: 0.7994\n",
      "Epoch [7/8], Loss: 0.7941\n",
      "Epoch [8/8], Loss: 0.7939\n",
      "Epoch 9/30, Val Loss: 0.7768\n",
      "Epoch [1/9], Loss: 0.7984\n",
      "Epoch [2/9], Loss: 0.7906\n",
      "Epoch [3/9], Loss: 0.7934\n",
      "Epoch [4/9], Loss: 0.7855\n",
      "Epoch [5/9], Loss: 0.7865\n",
      "Epoch [6/9], Loss: 0.7924\n",
      "Epoch [7/9], Loss: 0.7876\n",
      "Epoch [8/9], Loss: 0.7892\n",
      "Epoch [9/9], Loss: 0.7870\n",
      "Epoch 10/30, Val Loss: 0.7646\n",
      "Epoch [1/10], Loss: 0.7788\n",
      "Epoch [2/10], Loss: 0.7791\n",
      "Epoch [3/10], Loss: 0.7795\n",
      "Epoch [4/10], Loss: 0.7807\n",
      "Epoch [5/10], Loss: 0.7722\n",
      "Epoch [6/10], Loss: 0.7758\n",
      "Epoch [7/10], Loss: 0.7726\n",
      "Epoch [8/10], Loss: 0.7764\n",
      "Epoch [9/10], Loss: 0.7686\n",
      "Epoch [10/10], Loss: 0.7690\n",
      "Epoch 11/30, Val Loss: 0.7502\n",
      "Epoch [1/11], Loss: 0.7707\n",
      "Epoch [2/11], Loss: 0.7663\n",
      "Epoch [3/11], Loss: 0.7659\n",
      "Epoch [4/11], Loss: 0.7601\n",
      "Epoch [5/11], Loss: 0.7627\n",
      "Epoch [6/11], Loss: 0.7604\n",
      "Epoch [7/11], Loss: 0.7562\n",
      "Epoch [8/11], Loss: 0.7592\n",
      "Epoch [9/11], Loss: 0.7538\n",
      "Epoch [10/11], Loss: 0.7519\n",
      "Epoch [11/11], Loss: 0.7493\n",
      "Epoch 12/30, Val Loss: 0.7332\n",
      "Epoch [1/12], Loss: 0.7491\n",
      "Epoch [2/12], Loss: 0.7494\n",
      "Epoch [3/12], Loss: 0.7462\n",
      "Epoch [4/12], Loss: 0.7474\n",
      "Epoch [5/12], Loss: 0.7424\n",
      "Epoch [6/12], Loss: 0.7411\n",
      "Epoch [7/12], Loss: 0.7409\n",
      "Epoch [8/12], Loss: 0.7355\n",
      "Epoch [9/12], Loss: 0.7381\n",
      "Epoch [10/12], Loss: 0.7385\n",
      "Epoch [11/12], Loss: 0.7343\n",
      "Epoch [12/12], Loss: 0.7303\n",
      "Epoch 13/30, Val Loss: 0.7128\n",
      "Epoch [1/13], Loss: 0.7292\n",
      "Epoch [2/13], Loss: 0.7281\n",
      "Epoch [3/13], Loss: 0.7264\n",
      "Epoch [4/13], Loss: 0.7221\n",
      "Epoch [5/13], Loss: 0.7208\n",
      "Epoch [6/13], Loss: 0.7195\n",
      "Epoch [7/13], Loss: 0.7157\n",
      "Epoch [8/13], Loss: 0.7159\n",
      "Epoch [9/13], Loss: 0.7128\n",
      "Epoch [10/13], Loss: 0.7149\n",
      "Epoch [11/13], Loss: 0.7117\n",
      "Epoch [12/13], Loss: 0.7107\n",
      "Epoch [13/13], Loss: 0.7040\n",
      "Epoch 14/30, Val Loss: 0.6881\n",
      "Epoch [1/14], Loss: 0.7037\n",
      "Epoch [2/14], Loss: 0.7018\n",
      "Epoch [3/14], Loss: 0.7021\n",
      "Epoch [4/14], Loss: 0.6956\n",
      "Epoch [5/14], Loss: 0.6937\n",
      "Epoch [6/14], Loss: 0.6979\n",
      "Epoch [7/14], Loss: 0.6943\n",
      "Epoch [8/14], Loss: 0.6914\n",
      "Epoch [9/14], Loss: 0.6850\n",
      "Epoch [10/14], Loss: 0.6845\n",
      "Epoch [11/14], Loss: 0.6823\n",
      "Epoch [12/14], Loss: 0.6791\n",
      "Epoch [13/14], Loss: 0.6793\n",
      "Epoch [14/14], Loss: 0.6772\n",
      "Epoch 15/30, Val Loss: 0.6572\n",
      "Epoch [1/15], Loss: 0.6788\n",
      "Epoch [2/15], Loss: 0.6747\n",
      "Epoch [3/15], Loss: 0.6706\n",
      "Epoch [4/15], Loss: 0.6675\n",
      "Epoch [5/15], Loss: 0.6666\n",
      "Epoch [6/15], Loss: 0.6603\n",
      "Epoch [7/15], Loss: 0.6569\n",
      "Epoch [8/15], Loss: 0.6600\n",
      "Epoch [9/15], Loss: 0.6517\n",
      "Epoch [10/15], Loss: 0.6497\n",
      "Epoch [11/15], Loss: 0.6497\n",
      "Epoch [12/15], Loss: 0.6452\n",
      "Epoch [13/15], Loss: 0.6468\n",
      "Epoch [14/15], Loss: 0.6391\n",
      "Epoch [15/15], Loss: 0.6390\n",
      "Epoch 16/30, Val Loss: 0.6166\n",
      "Epoch [1/16], Loss: 0.6368\n",
      "Epoch [2/16], Loss: 0.6263\n",
      "Epoch [3/16], Loss: 0.6256\n",
      "Epoch [4/16], Loss: 0.6233\n",
      "Epoch [5/16], Loss: 0.6201\n",
      "Epoch [6/16], Loss: 0.6199\n",
      "Epoch [7/16], Loss: 0.6173\n",
      "Epoch [8/16], Loss: 0.6131\n",
      "Epoch [9/16], Loss: 0.6091\n",
      "Epoch [10/16], Loss: 0.6057\n",
      "Epoch [11/16], Loss: 0.6005\n",
      "Epoch [12/16], Loss: 0.5975\n",
      "Epoch [13/16], Loss: 0.5933\n",
      "Epoch [14/16], Loss: 0.5895\n",
      "Epoch [15/16], Loss: 0.5848\n",
      "Epoch [16/16], Loss: 0.5814\n",
      "Epoch 17/30, Val Loss: 0.5608\n",
      "Epoch [1/17], Loss: 0.5791\n",
      "Epoch [2/17], Loss: 0.5719\n",
      "Epoch [3/17], Loss: 0.5683\n",
      "Epoch [4/17], Loss: 0.5669\n",
      "Epoch [5/17], Loss: 0.5608\n",
      "Epoch [6/17], Loss: 0.5545\n",
      "Epoch [7/17], Loss: 0.5519\n",
      "Epoch [8/17], Loss: 0.5470\n",
      "Epoch [9/17], Loss: 0.5398\n",
      "Epoch [10/17], Loss: 0.5398\n",
      "Epoch [11/17], Loss: 0.5348\n",
      "Epoch [12/17], Loss: 0.5280\n",
      "Epoch [13/17], Loss: 0.5181\n",
      "Epoch [14/17], Loss: 0.5173\n",
      "Epoch [15/17], Loss: 0.5106\n",
      "Epoch [16/17], Loss: 0.5077\n",
      "Epoch [17/17], Loss: 0.5004\n",
      "Epoch 18/30, Val Loss: 0.4790\n",
      "Epoch [1/18], Loss: 0.4923\n",
      "Epoch [2/18], Loss: 0.4917\n",
      "Epoch [3/18], Loss: 0.4847\n",
      "Epoch [4/18], Loss: 0.4808\n",
      "Epoch [5/18], Loss: 0.4734\n",
      "Epoch [6/18], Loss: 0.4632\n",
      "Epoch [7/18], Loss: 0.4583\n",
      "Epoch [8/18], Loss: 0.4516\n",
      "Epoch [9/18], Loss: 0.4425\n",
      "Epoch [10/18], Loss: 0.4383\n",
      "Epoch [11/18], Loss: 0.4320\n",
      "Epoch [12/18], Loss: 0.4251\n",
      "Epoch [13/18], Loss: 0.4123\n",
      "Epoch [14/18], Loss: 0.4095\n",
      "Epoch [15/18], Loss: 0.4017\n",
      "Epoch [16/18], Loss: 0.3918\n",
      "Epoch [17/18], Loss: 0.3856\n",
      "Epoch [18/18], Loss: 0.3738\n",
      "Epoch 19/30, Val Loss: 0.3500\n",
      "Epoch [1/19], Loss: 0.3666\n",
      "Epoch [2/19], Loss: 0.3557\n",
      "Epoch [3/19], Loss: 0.3490\n",
      "Epoch [4/19], Loss: 0.3356\n",
      "Epoch [5/19], Loss: 0.3301\n",
      "Epoch [6/19], Loss: 0.3169\n",
      "Epoch [7/19], Loss: 0.3098\n",
      "Epoch [8/19], Loss: 0.3000\n",
      "Epoch [9/19], Loss: 0.2855\n",
      "Epoch [10/19], Loss: 0.2772\n",
      "Epoch [11/19], Loss: 0.2603\n",
      "Epoch [12/19], Loss: 0.2523\n",
      "Epoch [13/19], Loss: 0.2386\n",
      "Epoch [14/19], Loss: 0.2243\n",
      "Epoch [15/19], Loss: 0.2156\n",
      "Epoch [16/19], Loss: 0.2006\n",
      "Epoch [17/19], Loss: 0.1865\n",
      "Epoch [18/19], Loss: 0.1800\n",
      "Epoch [19/19], Loss: 0.1606\n",
      "Epoch 20/30, Val Loss: 0.1305\n",
      "Epoch [1/20], Loss: 0.1437\n",
      "Epoch [2/20], Loss: 0.1343\n",
      "Epoch [3/20], Loss: 0.1161\n",
      "Epoch [4/20], Loss: 0.1027\n",
      "Epoch [5/20], Loss: 0.0933\n",
      "Epoch [6/20], Loss: 0.0945\n",
      "Epoch [7/20], Loss: 0.0955\n",
      "Epoch [8/20], Loss: 0.0949\n",
      "Epoch [9/20], Loss: 0.0986\n",
      "Epoch [10/20], Loss: 0.1002\n",
      "Epoch [11/20], Loss: 0.0994\n",
      "Epoch [12/20], Loss: 0.0995\n",
      "Epoch [13/20], Loss: 0.0983\n",
      "Epoch [14/20], Loss: 0.0977\n",
      "Epoch [15/20], Loss: 0.0966\n",
      "Epoch [16/20], Loss: 0.0980\n",
      "Epoch [17/20], Loss: 0.0971\n",
      "Epoch [18/20], Loss: 0.0959\n",
      "Epoch [19/20], Loss: 0.0937\n",
      "Epoch [20/20], Loss: 0.0926\n",
      "Epoch 21/30, Val Loss: 0.0768\n",
      "Epoch [1/21], Loss: 0.0936\n",
      "Epoch [2/21], Loss: 0.0925\n",
      "Epoch [3/21], Loss: 0.0926\n",
      "Epoch [4/21], Loss: 0.0927\n",
      "Epoch [5/21], Loss: 0.0899\n",
      "Epoch [6/21], Loss: 0.0889\n",
      "Epoch [7/21], Loss: 0.0926\n",
      "Epoch [8/21], Loss: 0.0902\n",
      "Epoch [9/21], Loss: 0.0891\n",
      "Epoch [10/21], Loss: 0.0918\n",
      "Epoch [11/21], Loss: 0.0947\n",
      "Epoch [12/21], Loss: 0.0892\n",
      "Epoch [13/21], Loss: 0.0905\n",
      "Epoch [14/21], Loss: 0.0908\n",
      "Epoch [15/21], Loss: 0.0909\n",
      "Epoch [16/21], Loss: 0.0934\n",
      "Epoch [17/21], Loss: 0.0883\n",
      "Epoch [18/21], Loss: 0.0892\n",
      "Epoch [19/21], Loss: 0.0877\n",
      "Epoch [20/21], Loss: 0.0908\n",
      "Epoch [21/21], Loss: 0.0903\n",
      "Epoch 22/30, Val Loss: 0.0711\n",
      "Epoch [1/22], Loss: 0.0916\n",
      "Epoch [2/22], Loss: 0.0865\n",
      "Epoch [3/22], Loss: 0.0877\n",
      "Epoch [4/22], Loss: 0.0896\n",
      "Epoch [5/22], Loss: 0.0875\n",
      "Epoch [6/22], Loss: 0.0914\n",
      "Epoch [7/22], Loss: 0.0918\n",
      "Epoch [8/22], Loss: 0.0877\n",
      "Epoch [9/22], Loss: 0.0884\n",
      "Epoch [10/22], Loss: 0.0912\n",
      "Epoch [11/22], Loss: 0.0868\n",
      "Epoch [12/22], Loss: 0.0862\n",
      "Epoch [13/22], Loss: 0.0882\n",
      "Epoch [14/22], Loss: 0.0919\n",
      "Epoch [15/22], Loss: 0.0888\n",
      "Epoch [16/22], Loss: 0.0885\n",
      "Epoch [17/22], Loss: 0.0884\n",
      "Epoch [18/22], Loss: 0.0847\n",
      "Epoch [19/22], Loss: 0.0864\n",
      "Epoch [20/22], Loss: 0.0863\n",
      "Epoch [21/22], Loss: 0.0852\n",
      "Epoch [22/22], Loss: 0.0868\n",
      "Epoch 23/30, Val Loss: 0.0691\n",
      "Epoch [1/23], Loss: 0.0849\n",
      "Epoch [2/23], Loss: 0.0870\n",
      "Epoch [3/23], Loss: 0.0852\n",
      "Epoch [4/23], Loss: 0.0882\n",
      "Epoch [5/23], Loss: 0.0856\n",
      "Epoch [6/23], Loss: 0.0841\n",
      "Epoch [7/23], Loss: 0.0861\n",
      "Epoch [8/23], Loss: 0.0815\n",
      "Epoch [9/23], Loss: 0.0880\n",
      "Epoch [10/23], Loss: 0.0910\n",
      "Epoch [11/23], Loss: 0.0817\n",
      "Epoch [12/23], Loss: 0.0880\n",
      "Epoch [13/23], Loss: 0.0843\n",
      "Epoch [14/23], Loss: 0.0852\n",
      "Epoch [15/23], Loss: 0.0868\n",
      "Epoch [16/23], Loss: 0.0839\n",
      "Epoch [17/23], Loss: 0.0839\n",
      "Epoch [18/23], Loss: 0.0832\n",
      "Epoch [19/23], Loss: 0.0849\n",
      "Epoch [20/23], Loss: 0.0848\n",
      "Epoch [21/23], Loss: 0.0854\n",
      "Epoch [22/23], Loss: 0.0829\n",
      "Epoch [23/23], Loss: 0.0853\n",
      "Epoch 24/30, Val Loss: 0.0674\n",
      "Epoch [1/24], Loss: 0.0831\n",
      "Epoch [2/24], Loss: 0.0849\n",
      "Epoch [3/24], Loss: 0.0857\n",
      "Epoch [4/24], Loss: 0.0837\n",
      "Epoch [5/24], Loss: 0.0849\n",
      "Epoch [6/24], Loss: 0.0835\n",
      "Epoch [7/24], Loss: 0.0808\n",
      "Epoch [8/24], Loss: 0.0856\n",
      "Epoch [9/24], Loss: 0.0829\n",
      "Epoch [10/24], Loss: 0.0853\n",
      "Epoch [11/24], Loss: 0.0822\n",
      "Epoch [12/24], Loss: 0.0813\n",
      "Epoch [13/24], Loss: 0.0845\n",
      "Epoch [14/24], Loss: 0.0828\n",
      "Epoch [15/24], Loss: 0.0840\n",
      "Epoch [16/24], Loss: 0.0841\n",
      "Epoch [17/24], Loss: 0.0858\n",
      "Epoch [18/24], Loss: 0.0816\n",
      "Epoch [19/24], Loss: 0.0854\n",
      "Epoch [20/24], Loss: 0.0829\n",
      "Epoch [21/24], Loss: 0.0819\n",
      "Epoch [22/24], Loss: 0.0808\n",
      "Epoch [23/24], Loss: 0.0763\n",
      "Epoch [24/24], Loss: 0.0830\n",
      "Epoch 25/30, Val Loss: 0.0657\n",
      "Epoch [1/25], Loss: 0.0828\n",
      "Epoch [2/25], Loss: 0.0809\n",
      "Epoch [3/25], Loss: 0.0784\n",
      "Epoch [4/25], Loss: 0.0820\n",
      "Epoch [5/25], Loss: 0.0799\n",
      "Epoch [6/25], Loss: 0.0788\n",
      "Epoch [7/25], Loss: 0.0803\n",
      "Epoch [8/25], Loss: 0.0812\n",
      "Epoch [9/25], Loss: 0.0784\n",
      "Epoch [10/25], Loss: 0.0809\n",
      "Epoch [11/25], Loss: 0.0838\n",
      "Epoch [12/25], Loss: 0.0792\n",
      "Epoch [13/25], Loss: 0.0807\n",
      "Epoch [14/25], Loss: 0.0770\n",
      "Epoch [15/25], Loss: 0.0804\n",
      "Epoch [16/25], Loss: 0.0812\n",
      "Epoch [17/25], Loss: 0.0854\n",
      "Epoch [18/25], Loss: 0.0781\n",
      "Epoch [19/25], Loss: 0.0820\n",
      "Epoch [20/25], Loss: 0.0778\n",
      "Epoch [21/25], Loss: 0.0797\n",
      "Epoch [22/25], Loss: 0.0798\n",
      "Epoch [23/25], Loss: 0.0765\n",
      "Epoch [24/25], Loss: 0.0792\n",
      "Epoch [25/25], Loss: 0.0790\n",
      "Epoch 26/30, Val Loss: 0.0641\n",
      "Epoch [1/26], Loss: 0.0823\n",
      "Epoch [2/26], Loss: 0.0789\n",
      "Epoch [3/26], Loss: 0.0773\n",
      "Epoch [4/26], Loss: 0.0806\n",
      "Epoch [5/26], Loss: 0.0776\n",
      "Epoch [6/26], Loss: 0.0785\n",
      "Epoch [7/26], Loss: 0.0763\n",
      "Epoch [8/26], Loss: 0.0779\n",
      "Epoch [9/26], Loss: 0.0778\n",
      "Epoch [10/26], Loss: 0.0762\n",
      "Epoch [11/26], Loss: 0.0778\n",
      "Epoch [12/26], Loss: 0.0803\n",
      "Epoch [13/26], Loss: 0.0798\n",
      "Epoch [14/26], Loss: 0.0747\n",
      "Epoch [15/26], Loss: 0.0794\n",
      "Epoch [16/26], Loss: 0.0776\n",
      "Epoch [17/26], Loss: 0.0766\n",
      "Epoch [18/26], Loss: 0.0740\n",
      "Epoch [19/26], Loss: 0.0770\n",
      "Epoch [20/26], Loss: 0.0731\n",
      "Epoch [21/26], Loss: 0.0750\n",
      "Epoch [22/26], Loss: 0.0760\n",
      "Epoch [23/26], Loss: 0.0796\n",
      "Epoch [24/26], Loss: 0.0777\n",
      "Epoch [25/26], Loss: 0.0779\n",
      "Epoch [26/26], Loss: 0.0752\n",
      "Epoch 27/30, Val Loss: 0.0626\n",
      "Epoch [1/27], Loss: 0.0789\n",
      "Epoch [2/27], Loss: 0.0740\n",
      "Epoch [3/27], Loss: 0.0753\n",
      "Epoch [4/27], Loss: 0.0742\n",
      "Epoch [5/27], Loss: 0.0757\n",
      "Epoch [6/27], Loss: 0.0742\n",
      "Epoch [7/27], Loss: 0.0753\n",
      "Epoch [8/27], Loss: 0.0755\n",
      "Epoch [9/27], Loss: 0.0732\n",
      "Epoch [10/27], Loss: 0.0719\n",
      "Epoch [11/27], Loss: 0.0738\n",
      "Epoch [12/27], Loss: 0.0758\n",
      "Epoch [13/27], Loss: 0.0734\n",
      "Epoch [14/27], Loss: 0.0740\n",
      "Epoch [15/27], Loss: 0.0738\n",
      "Epoch [16/27], Loss: 0.0757\n",
      "Epoch [17/27], Loss: 0.0729\n",
      "Epoch [18/27], Loss: 0.0725\n",
      "Epoch [19/27], Loss: 0.0741\n",
      "Epoch [20/27], Loss: 0.0740\n",
      "Epoch [21/27], Loss: 0.0762\n",
      "Epoch [22/27], Loss: 0.0747\n",
      "Epoch [23/27], Loss: 0.0768\n",
      "Epoch [24/27], Loss: 0.0754\n",
      "Epoch [25/27], Loss: 0.0718\n",
      "Epoch [26/27], Loss: 0.0735\n",
      "Epoch [27/27], Loss: 0.0731\n",
      "Epoch 28/30, Val Loss: 0.0609\n",
      "Epoch [1/28], Loss: 0.0705\n",
      "Epoch [2/28], Loss: 0.0726\n",
      "Epoch [3/28], Loss: 0.0762\n",
      "Epoch [4/28], Loss: 0.0715\n",
      "Epoch [5/28], Loss: 0.0693\n",
      "Epoch [6/28], Loss: 0.0737\n",
      "Epoch [7/28], Loss: 0.0716\n",
      "Epoch [8/28], Loss: 0.0771\n",
      "Epoch [9/28], Loss: 0.0735\n",
      "Epoch [10/28], Loss: 0.0714\n",
      "Epoch [11/28], Loss: 0.0752\n",
      "Epoch [12/28], Loss: 0.0735\n",
      "Epoch [13/28], Loss: 0.0730\n",
      "Epoch [14/28], Loss: 0.0701\n",
      "Epoch [15/28], Loss: 0.0721\n",
      "Epoch [16/28], Loss: 0.0758\n",
      "Epoch [17/28], Loss: 0.0733\n",
      "Epoch [18/28], Loss: 0.0717\n",
      "Epoch [19/28], Loss: 0.0726\n",
      "Epoch [20/28], Loss: 0.0749\n",
      "Epoch [21/28], Loss: 0.0721\n",
      "Epoch [22/28], Loss: 0.0737\n",
      "Epoch [23/28], Loss: 0.0705\n",
      "Epoch [24/28], Loss: 0.0735\n",
      "Epoch [25/28], Loss: 0.0700\n",
      "Epoch [26/28], Loss: 0.0711\n",
      "Epoch [27/28], Loss: 0.0706\n",
      "Epoch [28/28], Loss: 0.0686\n",
      "Epoch 29/30, Val Loss: 0.0591\n",
      "Epoch [1/29], Loss: 0.0677\n",
      "Epoch [2/29], Loss: 0.0679\n",
      "Epoch [3/29], Loss: 0.0700\n",
      "Epoch [4/29], Loss: 0.0686\n",
      "Epoch [5/29], Loss: 0.0713\n",
      "Epoch [6/29], Loss: 0.0725\n",
      "Epoch [7/29], Loss: 0.0704\n",
      "Epoch [8/29], Loss: 0.0677\n",
      "Epoch [9/29], Loss: 0.0683\n",
      "Epoch [10/29], Loss: 0.0699\n",
      "Epoch [11/29], Loss: 0.0703\n",
      "Epoch [12/29], Loss: 0.0698\n",
      "Epoch [13/29], Loss: 0.0681\n",
      "Epoch [14/29], Loss: 0.0692\n",
      "Epoch [15/29], Loss: 0.0688\n",
      "Epoch [16/29], Loss: 0.0701\n",
      "Epoch [17/29], Loss: 0.0678\n",
      "Epoch [18/29], Loss: 0.0696\n",
      "Epoch [19/29], Loss: 0.0701\n",
      "Epoch [20/29], Loss: 0.0711\n",
      "Epoch [21/29], Loss: 0.0704\n",
      "Epoch [22/29], Loss: 0.0678\n",
      "Epoch [23/29], Loss: 0.0685\n",
      "Epoch [24/29], Loss: 0.0697\n",
      "Epoch [25/29], Loss: 0.0694\n",
      "Epoch [26/29], Loss: 0.0671\n",
      "Epoch [27/29], Loss: 0.0674\n",
      "Epoch [28/29], Loss: 0.0681\n",
      "Epoch [29/29], Loss: 0.0684\n",
      "Epoch 30/30, Val Loss: 0.0567\n",
      "Best Validation Loss for 건고추: 0.0567\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lstm_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(lstm_labels), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBM 학습을 위한 피처와 라벨의 길이가 일치하지 않습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# LGBM 모델 학습\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m lgbm_model \u001b[38;5;241m=\u001b[39m train_lgbm(lstm_features, lstm_labels, test_loader\u001b[38;5;241m=\u001b[39mval_loader)\n\u001b[0;32m     60\u001b[0m 품목_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m### 추론\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 66\u001b[0m, in \u001b[0;36mtrain_lgbm\u001b[1;34m(lstm_model, train_loader, test_loader, num_boost_round, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_lgbm\u001b[39m(lstm_model, train_loader, test_loader, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# LSTM을 사용해 시계열 데이터를 처리하고 특징을 추출\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m prepare_hybrid_data(lstm_model, train_loader)\n\u001b[0;32m     67\u001b[0m     X_test, y_test \u001b[38;5;241m=\u001b[39m prepare_hybrid_data(lstm_model, test_loader)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Ensure y_train and y_test are 1-D arrays\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 47\u001b[0m, in \u001b[0;36mprepare_hybrid_data\u001b[1;34m(lstm_model, dataloader)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_hybrid_data\u001b[39m(lstm_model, dataloader):\n\u001b[1;32m---> 47\u001b[0m     lstm_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     48\u001b[0m     lstm_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     49\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "pbar_outer = tqdm(품목_리스트, desc=\"품목 처리 중\", position=0)\n",
    "for 품목명 in pbar_outer:\n",
    "    pbar_outer.set_description(f\"품목별 전처리 및 모델 학습 -> {품목명}\")\n",
    "    \n",
    "    # 데이터 전처리 및 스케일링\n",
    "    train_data, scaler = process_data(\"./Data/train/train.csv\", \n",
    "                                      \"./Data/train/meta/TRAIN_산지공판장_2018-2021.csv\", \n",
    "                                      \"./Data/train/meta/TRAIN_전국도매_2018-2021.csv\", \n",
    "                                      품목명)\n",
    "    품목별_scalers[품목명] = scaler\n",
    "    dataset = AgriculturePriceDataset(train_data)\n",
    "    \n",
    "    # train과 validation으로 분할\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, CFG.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, CFG.batch_size, shuffle=False)\n",
    "\n",
    "    input_size = len(dataset.numeric_columns)\n",
    "    \n",
    "    # LSTM 모델 생성\n",
    "    lstm_model = PricePredictionLSTM(input_size, CFG.hidden_size, CFG.num_layers, lstm_output_size=1)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(lstm_model.parameters(), CFG.learning_rate)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # LSTM 학습\n",
    "    lstm_features, lstm_labels = [], []\n",
    "    for epoch in range(CFG.epoch):\n",
    "        lstm_feat_epoch, lstm_labels_epoch = train_hybrid_model(lstm_model, train_loader, epoch, criterion, optimizer)\n",
    "        \n",
    "        # 피처와 라벨의 길이 체크\n",
    "        assert len(lstm_feat_epoch) == len(lstm_labels_epoch), \"피처와 라벨의 길이가 일치하지 않습니다.\"\n",
    "\n",
    "        val_loss = evaluate_model(lstm_model, val_loader, criterion)\n",
    "\n",
    "        lstm_features.extend(lstm_feat_epoch)\n",
    "        lstm_labels.extend(lstm_labels_epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(lstm_model.state_dict(), f'models/best_lstm_model_{품목명}.pth')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{CFG.epoch}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    print(f'Best Validation Loss for {품목명}: {best_val_loss:.4f}')\n",
    "\n",
    "    # LSTM 모델의 피처와 라벨을 사용하여 LGBM 학습\n",
    "    lstm_features = np.concatenate(lstm_features)\n",
    "    lstm_labels = np.concatenate(lstm_labels)\n",
    "\n",
    "    # 학습용 피처와 라벨 길이 확인\n",
    "    assert len(lstm_features) == len(lstm_labels), \"LGBM 학습을 위한 피처와 라벨의 길이가 일치하지 않습니다.\"\n",
    "\n",
    "    # LGBM 모델 학습\n",
    "    lgbm_model = train_lgbm(lstm_features, lstm_labels, test_loader=val_loader)\n",
    "    \n",
    "    품목_predictions = []\n",
    "\n",
    "    ### 추론\n",
    "    pbar_inner = tqdm(range(25), desc=\"테스트 파일 추론 중\", position=1, leave=False)\n",
    "    for i in pbar_inner:\n",
    "        test_file = f\"./Data/test/TEST_{i:02d}.csv\"\n",
    "        산지공판장_file = f\"./Data/test/meta/TEST_산지공판장_{i:02d}.csv\"\n",
    "        전국도매_file = f\"./Data/test/meta/TEST_전국도매_{i:02d}.csv\"\n",
    "\n",
    "        # 데이터 전처리 및 스케일링\n",
    "        test_data, _ = process_data(test_file, 산지공판장_file, 전국도매_file, 품목명, scaler=품목별_scalers[품목명])\n",
    "        test_dataset = AgriculturePriceDataset(test_data, is_test=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # LSTM 모델 평가 모드로 전환\n",
    "        if isinstance(lstm_model, torch.nn.Module):  # lstm_model이 torch.nn.Module인지 확인\n",
    "            lstm_model.eval()  # LSTM 모델을 평가 모드로 전환\n",
    "        else:\n",
    "            raise TypeError(\"lstm_model이 numpy 배열로 잘못 변환되었습니다. PyTorch 모델이어야 합니다.\")\n",
    "\n",
    "        lstm_test_features = []\n",
    "        \n",
    "        with torch.no_grad():  # 평가 시에는 no_grad로 메모리 절약\n",
    "            for batch in test_loader:\n",
    "                # 배치 데이터를 모델이 있는 디바이스로 전송 (GPU 또는 CPU)\n",
    "                batch = batch.to(next(lstm_model.parameters()).device)\n",
    "\n",
    "                # 모델 추론 (예측)\n",
    "                lstm_output = lstm_model(batch)\n",
    "                lstm_test_features.append(lstm_output.cpu().numpy())  # numpy로 변환하여 저장\n",
    "\n",
    "        # LGBM을 사용한 예측\n",
    "        lstm_test_features = np.concatenate(lstm_test_features)\n",
    "        predictions = lgbm_model.predict(lstm_test_features)\n",
    "\n",
    "        # 예측값을 원래 스케일로 복원\n",
    "        predictions_reshaped = predictions.reshape(-1, 1)\n",
    "\n",
    "        # 스케일링된 값을 원래 값으로 복원\n",
    "        price_column_index = test_data.columns.get_loc(test_dataset.price_column)\n",
    "        price_scaler = MinMaxScaler()\n",
    "        price_scaler.min_ = 품목별_scalers[품목명].min_[price_column_index]\n",
    "        price_scaler.scale_ = 품목별_scalers[품목명].scale_[price_column_index]\n",
    "        predictions_original_scale = price_scaler.inverse_transform(predictions_reshaped)\n",
    "\n",
    "        # NaN 값이 있는지 확인하고 처리\n",
    "        if np.isnan(predictions_original_scale).any():\n",
    "            pbar_inner.set_postfix({\"상태\": \"NaN\"})\n",
    "        else:\n",
    "            pbar_inner.set_postfix({\"상태\": \"정상\"})\n",
    "            품목_predictions.extend(predictions_original_scale.flatten())\n",
    "\n",
    "    품목별_predictions[품목명] = 품목_predictions\n",
    "    pbar_outer.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a8006f68045b6b2a5e0ff38e756f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "품목 처리 중:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 229\u001b[0m\n\u001b[0;32m    227\u001b[0m residuals \u001b[38;5;241m=\u001b[39m residuals\u001b[38;5;241m.\u001b[39mdropna()  \u001b[38;5;66;03m# ARIMA 초기 학습 기간 동안 생기는 NaN 값 제거\u001b[39;00m\n\u001b[0;32m    228\u001b[0m residuals_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(residuals\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 229\u001b[0m dataset \u001b[38;5;241m=\u001b[39m AgriculturePriceDataset(pd\u001b[38;5;241m.\u001b[39mDataFrame(residuals_scaled), is_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Train/Validation Split for LSTM\u001b[39;00m\n\u001b[0;32m    232\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m train_test_split(dataset, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[57], line 152\u001b[0m, in \u001b[0;36mAgriculturePriceDataset.__init__\u001b[1;34m(self, dataframe, window_size, prediction_length, is_test)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_length \u001b[38;5;241m=\u001b[39m prediction_length\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_test \u001b[38;5;241m=\u001b[39m is_test\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprice_column \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m평균가격(원)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(col\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumeric_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[57], line 152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_length \u001b[38;5;241m=\u001b[39m prediction_length\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_test \u001b[38;5;241m=\u001b[39m is_test\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprice_column \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m평균가격(원)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(col\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumeric_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from types import SimpleNamespace\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import os\n",
    "\n",
    "# Hyperparameter Setting\n",
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_size\": 3\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)\n",
    "\n",
    "품목_리스트 = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']\n",
    "\n",
    "\n",
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, scaler=None):\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "    # 타겟 및 메타데이터 필터 조건 정의\n",
    "    conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None, \n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    "\n",
    "        # 타겟 데이터 필터링\n",
    "    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "    target_mask = conditions[품목명]['target'](raw_품목)\n",
    "    filtered_data = raw_품목[target_mask]\n",
    "\n",
    "    # 다른 품종에 대한 파생변수 생성 (생략)\n",
    "    \n",
    "    # 공판장 데이터 처리\n",
    "    if conditions[품목명]['공판장'] is not None:  # None 체크\n",
    "        filtered_공판장 = 산지공판장\n",
    "        for key, value in conditions[품목명]['공판장'].items():\n",
    "            if value is not None:\n",
    "                filtered_공판장 = filtered_공판장[filtered_공판장[key].isin(value)]\n",
    "        \n",
    "        filtered_공판장 = filtered_공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "\n",
    "    # 도매 데이터 처리\n",
    "    if conditions[품목명]['도매'] is not None:  # None 체크\n",
    "        filtered_도매 = 전국도매\n",
    "        for key, value in conditions[품목명]['도매'].items():\n",
    "            if value is not None:\n",
    "                filtered_도매 = filtered_도매[filtered_도매[key].isin(value)]\n",
    "        \n",
    "        filtered_도매 = filtered_도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 수치형 컬럼 처리\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data = filtered_data[['시점'] + list(numeric_columns)]\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n",
    "\n",
    "    # 정규화 적용\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler\n",
    "\n",
    "\n",
    "# Define LSTM Model\n",
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define Dataset Class\n",
    "class AgriculturePriceDataset(Dataset):\n",
    "    def __init__(self, dataframe, window_size=9, prediction_length=3, is_test=False):\n",
    "        self.data = dataframe\n",
    "        self.window_size = window_size\n",
    "        self.prediction_length = prediction_length\n",
    "        self.is_test = is_test\n",
    "        self.price_column = [col for col in self.data.columns if '평균가격(원)' in col and len(col.split('_')) == 1][0]\n",
    "        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.sequences = []\n",
    "        if not self.is_test:\n",
    "            for i in range(len(self.data) - self.window_size - self.prediction_length + 1):\n",
    "                x = self.data[self.numeric_columns].iloc[i:i+self.window_size].values\n",
    "                y = self.data[self.price_column].iloc[i+self.window_size:i+self.window_size+self.prediction_length].values\n",
    "                self.sequences.append((x, y))\n",
    "        else:\n",
    "            self.sequences = [self.data[self.numeric_columns].values]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_test:\n",
    "            x, y = self.sequences[idx]\n",
    "            return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "        else:\n",
    "            return torch.FloatTensor(self.sequences[idx])\n",
    "\n",
    "# Train the LSTM model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "# Apply ARIMA model for time series forecasting\n",
    "def apply_arima(data, order=(5,1,0)):\n",
    "    arima_model = ARIMA(data, order=order)\n",
    "    arima_fit = arima_model.fit()\n",
    "    return arima_fit\n",
    "\n",
    "# Calculate residuals\n",
    "def calculate_residuals(actual, predicted):\n",
    "    residuals = actual - predicted\n",
    "    return residuals\n",
    "\n",
    "# Hybrid model combining ARIMA and LSTM\n",
    "pbar_outer = tqdm(품목_리스트, desc=\"품목 처리 중\", position=0)\n",
    "for 품목명 in pbar_outer:\n",
    "    pbar_outer.set_description(f\"품목별 전처리 및 모델 학습 -> {품목명}\")\n",
    "    train_data, scaler = process_data(\"./Data/train/train.csv\", \n",
    "                                      \"./Data/train/meta/TRAIN_산지공판장_2018-2021.csv\", \n",
    "                                      \"./Data/train/meta/TRAIN_전국도매_2018-2021.csv\", \n",
    "                                      품목명)\n",
    "    \n",
    "    # Split dataset for ARIMA\n",
    "    train_arima_data = train_data['평균가격(원)']\n",
    "    \n",
    "    # 1. ARIMA 모델 적용\n",
    "    arima_model = apply_arima(train_arima_data)\n",
    "    arima_predictions = arima_model.fittedvalues\n",
    "\n",
    "    # 2. 잔차 계산 (Residuals)\n",
    "    residuals = calculate_residuals(train_arima_data, arima_predictions)\n",
    "\n",
    "    # 3. LSTM 모델을 위한 데이터 전처리 (잔차 학습)\n",
    "    residuals = residuals.dropna()  # ARIMA 초기 학습 기간 동안 생기는 NaN 값 제거\n",
    "    residuals_scaled = scaler.fit_transform(residuals.values.reshape(-1, 1))\n",
    "    dataset = AgriculturePriceDataset(pd.DataFrame(residuals_scaled), is_test=False)\n",
    "    \n",
    "    # Train/Validation Split for LSTM\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, CFG.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, CFG.batch_size, shuffle=False)\n",
    "\n",
    "    input_size = len(dataset.numeric_columns)\n",
    "    \n",
    "    # LSTM 모델 생성\n",
    "    model = PricePredictionLSTM(input_size, CFG.hidden_size, CFG.num_layers, CFG.output_size)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), CFG.learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    for epoch in range(CFG.epoch):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, CFG.epoch)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'models/best_model_{품목명}.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{CFG.epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    print(f'Best Validation Loss for {품목명}: {best_val_loss:.4f}')\n",
    "\n",
    "    # 4. 최종 예측: ARIMA 예측 + LSTM 잔차 예측\n",
    "    test_file = f\"./Data/test/TEST_00.csv\"\n",
    "    test_data, _ = process_data(test_file, \"\", \"\", 품목명, scaler=scaler)\n",
    "    arima_test_predictions = arima_model.forecast(steps=len(test_data))\n",
    "\n",
    "    test_dataset = AgriculturePriceDataset(test_data, is_test=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    lstm_residuals = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            output = model(batch)\n",
    "            lstm_residuals.append(output.numpy())\n",
    "\n",
    "    lstm_residuals_array = np.concatenate(lstm_residuals)\n",
    "\n",
    "    # 최종 예측값 계산\n",
    "    final_predictions = arima_test_predictions + lstm_residuals_array\n",
    "\n",
    "    print(f'Final Predictions for {품목명}: {final_predictions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
